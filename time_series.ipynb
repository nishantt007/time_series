{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Forecasting Next Time Step"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''This function creates as many time series as requested (via the batch_size argument), each of length n_steps, and there is just one\n",
    "value per time step in each series (i.e., all series are univariate). The function returns a NumPy array of shape [batch size, time steps, 1], where each series is the sum of two sine waves of fixed amplitudes but random frequencies and phases, plus a bit of noise.'''\n",
    "\n",
    "def generate_time_series(batch_size, n_steps):\n",
    "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  #  wave 1\n",
    "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # +wave 2\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # +noise\n",
    "    return series[..., np.newaxis].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "n_steps = 50\n",
    "series = generate_time_series(10000, n_steps + 1)\n",
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]\n",
    "\n",
    "# X_train contains 7,000 time series (i.e., its shape is [7000, 50, 1])'.  Since we want to forecast a single value for each series, the targets are column vectors (e.g. y_train has a shape of [7000, 1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Methods"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1) Naive Forecasting\n",
    "Predicting the last value in each series."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "0.02037423"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = X_valid[:, -1]\n",
    "np.mean(keras.losses.mean_squared_error(y_valid, y_pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2) Linear Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[50, 1]),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.1332\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.0380\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.0222\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.0144\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.0104\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.0084\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.0074\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.0067\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.0062\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.0058\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.0054\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.0052\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.0049\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.0047\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.0046\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.0044\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.0043\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.0042\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.0041\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.0040\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1d10ff4a3e0>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0038\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.0038358382880687714"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3) Single Simple RNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "'It contains a single layer with a single neuron.(=1 given by us). Do not need to specify the length of the input sequence, since a RNN can process any number of time steps (hence first input dimension = None). By default, the SimpleRNN layer uses hyperbolic tangent activation function.'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
    "])\n",
    "\n",
    "'''It contains a single layer with a single neuron.(=1 given by us). Do not need to specify the length of the input sequence, since a RNN can process any number of time steps (hence first input dimension = None). By default, the SimpleRNN layer uses hyperbolic tangent activation function.'''"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "219/219 [==============================] - 4s 10ms/step - loss: 0.3817\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.3340\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.2903\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.2499\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.2131\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.1813\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.1560\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.1357\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.1196\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.1026\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.0825\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.0658\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.0522\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0411\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 3s 12ms/step - loss: 0.0321\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.0251\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.0199\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.0163\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.0140\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.0126\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1d11028f1c0>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X_train, y_train, epochs=20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 4ms/step - loss: 0.0122\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.012219041585922241"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4) Multilayered Simple RNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "'''Set return_sequences=True for all recurrent layers (except the last one, if you only care about the last output). If you don’t, they will output a 2D array (containing only the output of the last time step) instead of a 3D array (containing outputs for all time steps), and the next recurrent layer will complain that you are not feeding it sequences in the expected 3D format.'''\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None,1]),    # no of neurons = 20\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(1)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "''' RNN will mostly use the hidden states of the other recurrent layers to carry over all the information it needs from time step to time step, and it will not use the final layer’s hidden state very much as it's return_sequence if false. Moreover, a SimpleRNN layer uses the tanh activation function by default, the predicted values must lie within the range –1 to 1. But what if you want to use another activation function?\n",
    " For both these reasons, replace the output layer with a Dense layer. Also make sure to remove return_sequences=True from the second (now last) recurrent layer'''\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None,1]),\n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "219/219 [==============================] - 7s 21ms/step - loss: 0.0444\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0050\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0036\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0033\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0031\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0030\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0030\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0029\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0029\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 4s 21ms/step - loss: 0.0028\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0029\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0028\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0028\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0028\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0027\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0027\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 5s 21ms/step - loss: 0.0027\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 5s 23ms/step - loss: 0.0027\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 5s 22ms/step - loss: 0.0027\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 4s 20ms/step - loss: 0.0027\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1d11357e6b0>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X_train, y_train, epochs=20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 7ms/step - loss: 0.0027\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.0026811794377863407"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "Accuracy --> Stacked RNN > ANN > Single RNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Forecasting Several Time Steps Ahead\n",
    "Eg: We want to predict the next 10 values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Methods"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1) Multilayered Simple RNN with single output\n",
    "Use the model we already trained, make it predict the next value, then add that value to the inputs (acting as if this predicted value had actually occurred), and use the model again to predict the following value, and so on."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n"
     ]
    }
   ],
   "source": [
    "series = generate_time_series(1, n_steps + 10)              # generating actual data upto 10 steps ahead\n",
    "X_new, Y_new = series[:, :n_steps], series[:, n_steps:]     # X = instances from start to n-steps, Y from n_steps to n_steps + 10\n",
    "X = X_new\n",
    "for step_ahead in range(10):\n",
    "    y_pred_one = model.predict(X[:, step_ahead:])[:, np.newaxis, :]     # explained below\n",
    "    X = np.concatenate([X, y_pred_one], axis=1)     # axis=1 specifies that concatenation should be done along the second axis, which represents the time steps.\n",
    "\n",
    "## [:, step_ahead:]: means include all rows (instances) of X, and step_ahead: specifies the range of columns (time steps) to select from step_ahead until the end of the sequence\n",
    "## [:, np.newaxis, :]: This part of the code reshapes the predicted values to match the target shape. The np.newaxis adds a new axis (dimension) to the predictions, specifically as the second axis. The : on both sides ensures that all rows and columns of the predictions are included. This reshaping is necessary to ensure compatibility when concatenating the predicted values later.\n",
    "## y_pred_one will have the shape (num_instances, 1, num_features), where num_instances represents the number of rows in X, 1 represents the newly added axis, and num_features represents the number of features in the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "Y_pred = X[:, n_steps:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 731ms/step - loss: 0.1583\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.1583329141139984"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(Y_pred, Y_new)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2) Multilayered Simple RNN with 10 outputs\n",
    "Predict all 10 next values at once. We can still use a sequence-to-vector model, but it will output 10 values instead of 1. However, we first need to change the targets to be vectors containing the next 10 values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "series = generate_time_series(10000, n_steps + 10)\n",
    "X_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0] # (#instances, #time steps, #features / dimensionality)\n",
    "X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\n",
    "X_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0]\n",
    "\n",
    "# 0 in series[:, :,  ], indicates that it wants to extract the values of the first feature for the selected rows and columns.\n",
    "# Therefore, series[9000:, -10:, 0] is selecting the last 10 steps of the first feature (index 0) for the instances starting from the 9000th row until the end of the series dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(10)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 713ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_new)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3) Multilayer RNN with 10 output using TimeDistributed layer\n",
    "At time step 0 the model will output a vector containing the forecasts for time steps 1 to 10, then at time step 1 the model will forecast time steps 2 to 11, and so on. So each target must be a sequence of the same length as the input sequence, containing a 10-dimensional vector at each step."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "Y = np.empty((10000, n_steps, 10))  # (no of instances, no of time steps, dimensionality for each target is a sequence of 10D vectors)\n",
    "for step_ahead in range(1, 10 + 1):\n",
    "    Y[:, :, step_ahead - 1] = series[:, step_ahead:step_ahead + n_steps, 0]\n",
    "Y_train = Y[:7000]\n",
    "Y_valid = Y[7000:9000]\n",
    "Y_test = Y[9000:]\n",
    "\n",
    "# series[:, step_ahead:step_ahead + n_steps, 0] This selects a slice from the series dataset. The : before , indicates that all rows of series are included. step_ahead:step_ahead + n_steps specifies the range of columns (time steps) to select from step_ahead until step_ahead + n_steps. 0 at the end selects the values of the first feature or column.\n",
    "# The resulting Y array will have dimensions (num_instances, n_steps, 10), where num_instances represents the number of instances in the dataset. Each slice of Y at Y[:, :, step_ahead - 1] will contain the target values for the corresponding step ahead."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "'''To turn the model into a sequence-to-sequence model, set return_sequences=True in all recurrent layers (even the last one), and\n",
    "we must apply the output Dense layer at every time step. Keras offers a TimeDistributed layer for this very purpose: it wraps any layer (e.g., a Dense layer) and applies it at every time step of its input sequence. It does so, by reshaping the inputs so that each time step is treated as a separate instance (i.e., it reshapes the inputs from [batch size, time steps, input dimensions] to [batch size × time steps, input dimensions]; in this example, the number of input dimensions is 20 because the previous SimpleRNN layer has 20 units), then it runs the Dense layer, and finally it reshapes the outputs back to sequences (i.e., it reshapes the outputs from [batch size × time steps, output dimensions] to [batch size, time steps, output dimensions]; in this example the number of output dimensions is 10, since the Dense layer has 10 units).'''\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "\n",
    "# we could replace the last layer with just Dense(10)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "'''All outputs are needed during training, but only the output at the last time step is useful for predictions and for evaluation. So although we will rely on the MSE over all the outputs for training, we will use a custom metric for evaluation, to only compute the MSE over the output at the last time step'''\n",
    "\n",
    "def last_time_step_mse(Y_true, Y_pred):\n",
    "    return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[last_time_step_mse])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
